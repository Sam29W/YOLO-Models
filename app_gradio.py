import gradio as gr
from ultralytics import YOLO
from PIL import Image
import numpy as np
import cv2
import tempfile
import os
import json
from datetime import datetime
import time
import zipfile
from pathlib import Path
import threading
from collections import defaultdict

try:
    import pygame

    pygame.mixer.init()
    SOUND_AVAILABLE = True
except:
    SOUND_AVAILABLE = False
    print("‚ö†Ô∏è Sound not available on this system (disabled for cloud deployment)")

try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image as RLImage, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.enums import TA_CENTER, TA_LEFT
    from reportlab.lib import colors

    PDF_AVAILABLE = True
except:
    PDF_AVAILABLE = False
    print("‚ö†Ô∏è ReportLab not available - PDF export disabled")

try:
    from skimage import filters
    from scipy import ndimage

    HEATMAP_AVAILABLE = True
except:
    HEATMAP_AVAILABLE = False
    print("‚ö†Ô∏è scikit-image/scipy not available - Heatmap disabled")

try:
    import qrcode

    QR_AVAILABLE = True
except:
    QR_AVAILABLE = False
    print("‚ö†Ô∏è QR code generation not available")

# Initialize with default model
current_model = YOLO("yolo11n.pt")
model_cache = {"yolo11n.pt": current_model}

# Create history directory
HISTORY_DIR = Path("detection_history")
HISTORY_DIR.mkdir(exist_ok=True)

# Create alerts directory
ALERTS_DIR = Path("alert_logs")
ALERTS_DIR.mkdir(exist_ok=True)

# Global alert log
alert_log = []

# Available models
AVAILABLE_MODELS = ["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]

# Track history for trajectory drawing
track_history = defaultdict(lambda: [])


def load_model(model_name):
    """
    Load or retrieve cached YOLO model
    """
    global current_model, model_cache

    if model_name not in model_cache:
        print(f"üì• Loading {model_name}...")
        model_cache[model_name] = YOLO(model_name)
        print(f"‚úÖ {model_name} loaded successfully!")

    current_model = model_cache[model_name]
    return current_model


def draw_custom_boxes(image, results, confidence):
    """
    Draw custom colored bounding boxes on image
    """
    if isinstance(image, Image.Image):
        image = np.array(image)

    annotated = image.copy()

    color_map = {
        'person': (255, 100, 100),
        'car': (100, 100, 255),
        'truck': (100, 255, 255),
        'bus': (255, 165, 100),
        'bicycle': (255, 100, 255),
        'motorcycle': (200, 100, 255),
        'dog': (100, 255, 100),
        'cat': (150, 255, 150),
        'bird': (255, 200, 100),
        'bottle': (180, 180, 100),
        'cup': (255, 150, 150),
        'laptop': (100, 200, 200),
        'phone': (200, 100, 200),
        'chair': (150, 150, 255),
    }

    for box in results[0].boxes:
        class_id = int(box.cls[0])
        class_name = results[0].names[class_id]
        conf = float(box.conf[0])

        if conf < confidence:
            continue

        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        color = color_map.get(class_name, (255, 255, 255))

        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 3)

        label = f"{class_name}: {conf:.2f}"
        (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(annotated, (x1, y1 - label_h - 10), (x1 + label_w, y1), color, -1)
        cv2.putText(annotated, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    return annotated


def generate_heatmap(image, results, confidence):
    """
    Generate detection heatmap showing object density
    """
    if not HEATMAP_AVAILABLE:
        print("‚ö†Ô∏è Heatmap generation not available")
        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = image
        black = np.zeros_like(img_array)
        return black, img_array

    try:
        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = image.copy()

        height, width = img_array.shape[:2]

        # Create empty heatmap
        heatmap = np.zeros((height, width), dtype=np.float32)

        # Add Gaussian blobs for each detection
        for box in results[0].boxes:
            conf = float(box.conf[0])

            if conf < confidence:
                continue

            # Get bounding box center
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2

            # Calculate radius based on box size
            box_width = x2 - x1
            box_height = y2 - y1
            radius = int(max(box_width, box_height) * 0.8)

            # Create meshgrid for Gaussian
            y, x = np.ogrid[-radius:radius + 1, -radius:radius + 1]

            # Gaussian formula
            gaussian = np.exp(-(x * x + y * y) / (2.0 * (radius / 2) ** 2))

            # Add to heatmap (with bounds checking)
            y_min = max(0, center_y - radius)
            y_max = min(height, center_y + radius + 1)
            x_min = max(0, center_x - radius)
            x_max = min(width, center_x + radius + 1)

            g_y_min = max(0, radius - center_y)
            g_y_max = g_y_min + (y_max - y_min)
            g_x_min = max(0, radius - center_x)
            g_x_max = g_x_min + (x_max - x_min)

            heatmap[y_min:y_max, x_min:x_max] += gaussian[g_y_min:g_y_max, g_x_min:g_x_max] * conf

        # Normalize heatmap
        if heatmap.max() > 0:
            heatmap = heatmap / heatmap.max()

        # Apply Gaussian blur for smoother visualization
        heatmap = filters.gaussian(heatmap, sigma=20)

        # Normalize again after blur
        if heatmap.max() > 0:
            heatmap = heatmap / heatmap.max()

        # Convert to color heatmap (blue -> green -> yellow -> red)
        heatmap_color = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)

        # Create overlay
        overlay = cv2.addWeighted(img_array, 0.6, heatmap_color, 0.4, 0)

        return heatmap_color, overlay

    except Exception as e:
        print(f"‚ùå Heatmap generation error: {e}")
        # Return black images as fallback
        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = image
        black = np.zeros_like(img_array)
        return black, img_array


def generate_video_heatmap(video_path, confidence, model_name, progress=gr.Progress()):
    """
    Generate aggregate heatmap for video showing detection hotspots
    """
    if not HEATMAP_AVAILABLE:
        print("‚ö†Ô∏è Heatmap generation not available")
        return None, None

    try:
        load_model(model_name)

        cap = cv2.VideoCapture(video_path)

        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Initialize aggregate heatmap
        aggregate_heatmap = np.zeros((height, width), dtype=np.float32)

        frame_count = 0
        sample_frame = None

        # Process every Nth frame for performance
        frame_skip = max(1, total_frames // 100)  # Process max 100 frames

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # Skip frames for performance
            if frame_count % frame_skip != 0:
                continue

            progress(frame_count / total_frames, desc=f"Generating heatmap... {frame_count}/{total_frames}")

            # Save one frame for display
            if sample_frame is None:
                sample_frame = frame.copy()

            # Run detection
            results = current_model.predict(source=frame, conf=confidence, save=False, verbose=False)

            # Add detections to aggregate heatmap
            for box in results[0].boxes:
                conf = float(box.conf[0])

                if conf < confidence:
                    continue

                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2

                box_width = x2 - x1
                box_height = y2 - y1
                radius = int(max(box_width, box_height) * 0.8)

                y, x = np.ogrid[-radius:radius + 1, -radius:radius + 1]
                gaussian = np.exp(-(x * x + y * y) / (2.0 * (radius / 2) ** 2))

                y_min = max(0, center_y - radius)
                y_max = min(height, center_y + radius + 1)
                x_min = max(0, center_x - radius)
                x_max = min(width, center_x + radius + 1)

                g_y_min = max(0, radius - center_y)
                g_y_max = g_y_min + (y_max - y_min)
                g_x_min = max(0, radius - center_x)
                g_x_max = g_x_min + (x_max - x_min)

                aggregate_heatmap[y_min:y_max, x_min:x_max] += gaussian[g_y_min:g_y_max, g_x_min:g_x_max]

        cap.release()

        # Normalize
        if aggregate_heatmap.max() > 0:
            aggregate_heatmap = aggregate_heatmap / aggregate_heatmap.max()

        # Apply Gaussian blur
        aggregate_heatmap = filters.gaussian(aggregate_heatmap, sigma=25)

        if aggregate_heatmap.max() > 0:
            aggregate_heatmap = aggregate_heatmap / aggregate_heatmap.max()

        # Convert to color
        heatmap_color = cv2.applyColorMap((aggregate_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)

        # Create overlay if we have a sample frame
        if sample_frame is not None:
            overlay = cv2.addWeighted(sample_frame, 0.6, heatmap_color, 0.4, 0)
        else:
            overlay = heatmap_color

        progress(1.0, desc="Heatmap complete!")

        return heatmap_color, overlay

    except Exception as e:
        print(f"‚ùå Video heatmap error: {e}")
        return None, None


def track_objects_in_video(video_path, confidence, model_name, draw_trails, progress=gr.Progress()):
    """
    Track objects across video frames with trajectory visualization
    """
    try:
        global track_history
        track_history = defaultdict(lambda: [])

        load_model(model_name)

        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        temp_output = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
        output_path = temp_output.name

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_count = 0
        track_stats = defaultdict(lambda: {'first_seen': 0, 'last_seen': 0, 'frames': 0})
        unique_ids = set()

        start_time = time.time()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            progress(frame_count / total_frames, desc=f"Tracking frame {frame_count}/{total_frames}")

            # Run tracking (using BoT-SORT tracker)
            results = current_model.track(source=frame, conf=confidence, persist=True, verbose=False)

            annotated_frame = frame.copy()

            # Process tracks
            if results[0].boxes.id is not None:
                boxes = results[0].boxes.xyxy.cpu().numpy()
                track_ids = results[0].boxes.id.cpu().numpy().astype(int)
                classes = results[0].boxes.cls.cpu().numpy().astype(int)
                confidences = results[0].boxes.conf.cpu().numpy()

                for box, track_id, cls, conf in zip(boxes, track_ids, classes, confidences):
                    if conf < confidence:
                        continue

                    unique_ids.add(track_id)
                    x1, y1, x2, y2 = box.astype(int)
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)

                    # Update track history
                    track_history[track_id].append(center)
                    if len(track_history[track_id]) > 30:  # Keep last 30 points
                        track_history[track_id].pop(0)

                    # Update stats
                    if track_stats[track_id]['first_seen'] == 0:
                        track_stats[track_id]['first_seen'] = frame_count
                    track_stats[track_id]['last_seen'] = frame_count
                    track_stats[track_id]['frames'] += 1

                    # Generate color based on ID
                    color = tuple([int((track_id * 50) % 255),
                                   int((track_id * 100) % 255),
                                   int((track_id * 150) % 255)])

                    # Draw bounding box
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 3)

                    # Draw ID and class
                    class_name = results[0].names[cls]
                    label = f"ID:{track_id} {class_name}"
                    cv2.putText(annotated_frame, label, (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

                    # Draw trajectory trail
                    if draw_trails and len(track_history[track_id]) > 1:
                        points = np.array(track_history[track_id], dtype=np.int32).reshape((-1, 1, 2))
                        cv2.polylines(annotated_frame, [points], isClosed=False,
                                      color=color, thickness=3)

                        # Draw circle at current position
                        cv2.circle(annotated_frame, center, 5, color, -1)

            out.write(annotated_frame)

        cap.release()
        out.release()

        total_time = time.time() - start_time

        # Generate summary
        summary = f"# üéØ Object Tracking Complete!\n\n"
        summary += f"**ü§ñ Model:** {model_name}\n"
        summary += f"**‚è±Ô∏è Processing Time:** {total_time:.2f}s\n"
        summary += f"**üìπ Frames Processed:** {frame_count}\n"
        summary += f"**üéØ Unique Objects Tracked:** {len(unique_ids)}\n\n"

        summary += "### üìä Track Statistics:\n\n"
        for track_id in sorted(unique_ids)[:10]:  # Show top 10
            stats = track_stats[track_id]
            duration = (stats['last_seen'] - stats['first_seen']) / fps
            summary += f"- **ID {track_id}:** Tracked for {duration:.1f}s ({stats['frames']} frames)\n"

        if len(unique_ids) > 10:
            summary += f"\n*...and {len(unique_ids) - 10} more objects*\n"

        progress(1.0, desc="Tracking complete!")

        return output_path, summary

    except Exception as e:
        print(f"‚ùå Tracking error: {e}")
        return None, f"### ‚ö†Ô∏è Tracking Error\n\n{str(e)}"


def generate_qr_code(url):
    """
    Generate QR code for easy mobile access
    """
    if not QR_AVAILABLE:
        return None

    try:
        qr = qrcode.QRCode(
            version=1,
            error_correction=qrcode.constants.ERROR_CORRECT_L,
            box_size=10,
            border=4,
        )
        qr.add_data(url)
        qr.make(fit=True)

        img = qr.make_image(fill_color="black", back_color="white")

        # Convert to numpy array for Gradio
        img_array = np.array(img.convert('RGB'))

        return img_array
    except Exception as e:
        print(f"‚ùå QR generation error: {e}")
        return None


def create_mobile_snapshot(image, results, confidence):
    """
    Create mobile-optimized snapshot with larger text and icons
    """
    if isinstance(image, Image.Image):
        img_array = np.array(image)
    else:
        img_array = image.copy()

    height, width = img_array.shape[:2]

    # Create copy for annotation
    annotated = img_array.copy()

    # Count objects
    object_counts = {}
    for box in results[0].boxes:
        class_id = int(box.cls[0])
        class_name = results[0].names[class_id]
        conf = float(box.conf[0])

        if conf < confidence:
            continue

        if class_name in object_counts:
            object_counts[class_name] += 1
        else:
            object_counts[class_name] = 1

        # Draw boxes with THICKER lines for mobile
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)

        # Mobile-friendly colors
        color = (0, 255, 0)  # Green for visibility
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 5)  # Thicker lines

        # Larger text for mobile
        label = f"{class_name}"
        font_scale = 1.5  # Bigger text
        thickness = 3

        (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
        cv2.rectangle(annotated, (x1, y1 - label_h - 20), (x1 + label_w + 10, y1), color, -1)
        cv2.putText(annotated, label, (x1 + 5, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness)

    # Add mobile-friendly info overlay
    overlay = annotated.copy()
    total_objects = sum(object_counts.values())

    # Info box at top
    info_text = f"Objects: {total_objects}"
    cv2.rectangle(overlay, (10, 10), (400, 100), (0, 0, 0), -1)
    cv2.putText(overlay, info_text, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)

    # Blend overlay
    annotated = cv2.addWeighted(annotated, 0.85, overlay, 0.15, 0)

    return annotated, object_counts


def detect_mobile_camera(image, confidence, model_name):
    """
    Mobile-optimized detection with larger UI elements
    """
    if image is None:
        return None, "### üì± Tap to capture photo!", None

    load_model(model_name)

    start_time = time.time()
    results = current_model.predict(source=image, conf=confidence, save=False, verbose=False)
    process_time = time.time() - start_time

    # Create mobile-optimized output
    annotated, object_counts = create_mobile_snapshot(image, results, confidence)

    # Generate summary
    total = sum(object_counts.values())
    summary = f"# üì± Mobile Detection\n\n"
    summary += f"**üéØ Found: {total} objects**\n"
    summary += f"**‚ö° Speed: {process_time:.2f}s**\n\n"

    if object_counts:
        summary += "### Objects:\n"
        emoji_map = {
            'person': 'üë§', 'car': 'üöó', 'truck': 'üöö', 'bus': 'üöå',
            'dog': 'üêï', 'cat': 'üêà', 'bicycle': 'üö≤', 'phone': 'üì±'
        }

        for obj, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
            emoji = emoji_map.get(obj, 'üì¶')
            summary += f"- {emoji} **{obj.capitalize()}: {count}**\n"

    # Save for download
    temp_file = None
    if total > 0:
        temp = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
        cv2.imwrite(temp.name, annotated)
        temp.close()
        temp_file = temp.name

    return annotated, summary, temp_file


# [KEEPING ALL YOUR EXISTING FUNCTIONS - PDF, alerts, history, etc. - UNCHANGED]
# Just adding the tracking function above

def generate_pdf_report(annotated_image, export_data, object_counts, metadata):
    """
    Generate professional PDF report with detection results
    """
    if not PDF_AVAILABLE:
        print("‚ö†Ô∏è PDF generation not available")
        return None

    try:
        pdf_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
        pdf_path = pdf_temp.name
        pdf_temp.close()

        doc = SimpleDocTemplate(pdf_path, pagesize=letter,
                                rightMargin=0.75 * inch, leftMargin=0.75 * inch,
                                topMargin=0.75 * inch, bottomMargin=0.75 * inch)

        elements = []

        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#2E86AB'),
            spaceAfter=30,
            alignment=TA_CENTER,
            fontName='Helvetica-Bold'
        )

        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=16,
            textColor=colors.HexColor('#A23B72'),
            spaceAfter=12,
            spaceBefore=12,
            fontName='Helvetica-Bold'
        )

        normal_style = styles['Normal']

        elements.append(Spacer(1, 1.5 * inch))
        elements.append(Paragraph("üéØ YOLO Object Detection Report", title_style))
        elements.append(Spacer(1, 0.3 * inch))

        elements.append(Paragraph(f"<b>Generated:</b> {metadata['timestamp']}", normal_style))
        elements.append(Paragraph(f"<b>Model:</b> {metadata['model']}", normal_style))
        elements.append(Paragraph(f"<b>Confidence Threshold:</b> {metadata['confidence']}", normal_style))
        elements.append(Spacer(1, 0.5 * inch))

        summary_data = [
            ['Total Objects', str(metadata['total_objects'])],
            ['Processing Time', f"{metadata['processing_time']:.2f}s"],
            ['FPS', f"{metadata['fps']:.1f}"],
            ['Unique Classes', str(len(object_counts))]
        ]

        summary_table = Table(summary_data, colWidths=[3 * inch, 2 * inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#2E86AB')),
            ('TEXTCOLOR', (0, 0), (0, -1), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 12),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
            ('TOPPADDING', (0, 0), (-1, -1), 12),
            ('BACKGROUND', (1, 0), (1, -1), colors.HexColor('#F18F01')),
            ('GRID', (0, 0), (-1, -1), 1, colors.grey)
        ]))

        elements.append(summary_table)
        elements.append(PageBreak())

        elements.append(Paragraph("Detection Results", heading_style))
        elements.append(Spacer(1, 0.2 * inch))

        img_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
        cv2.imwrite(img_temp.name, annotated_image)
        img_temp.close()

        img = RLImage(img_temp.name, width=6 * inch, height=4 * inch)
        elements.append(img)
        elements.append(Spacer(1, 0.3 * inch))

        elements.append(Paragraph("Object Summary", heading_style))
        elements.append(Spacer(1, 0.1 * inch))

        if object_counts:
            count_data = [['Object Type', 'Count', 'Percentage']]
            total = sum(object_counts.values())

            for obj, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total) * 100
                count_data.append([obj.capitalize(), str(count), f"{percentage:.1f}%"])

            count_table = Table(count_data, colWidths=[2.5 * inch, 1.5 * inch, 1.5 * inch])
            count_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2E86AB')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
                ('TOPPADDING', (0, 0), (-1, -1), 8),
                ('GRID', (0, 0), (-1, -1), 1, colors.grey),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#E8F4F8')])
            ]))

            elements.append(count_table)

        elements.append(PageBreak())

        elements.append(Paragraph("Detailed Detections", heading_style))
        elements.append(Spacer(1, 0.2 * inch))

        if export_data:
            detail_data = [['#', 'Object', 'Confidence', 'BBox (x1,y1,x2,y2)']]

            for idx, item in enumerate(export_data[:30], 1):
                bbox_str = f"({item['bbox_x1']}, {item['bbox_y1']}, {item['bbox_x2']}, {item['bbox_y2']})"
                detail_data.append([
                    str(idx),
                    item['object'].capitalize(),
                    item['confidence'],
                    bbox_str
                ])

            detail_table = Table(detail_data, colWidths=[0.5 * inch, 1.5 * inch, 1.2 * inch, 3.3 * inch])
            detail_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#A23B72')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 9),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
                ('TOPPADDING', (0, 0), (-1, -1), 6),
                ('GRID', (0, 0), (-1, -1), 1, colors.grey),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#FCE0EF')])
            ]))

            elements.append(detail_table)

            if len(export_data) > 30:
                elements.append(Spacer(1, 0.2 * inch))
                elements.append(Paragraph(
                    f"<i>Showing first 30 of {len(export_data)} total detections. See JSON/CSV for complete data.</i>",
                    normal_style))

        elements.append(Spacer(1, 0.5 * inch))
        footer_style = ParagraphStyle(
            'Footer',
            parent=styles['Normal'],
            fontSize=10,
            textColor=colors.grey,
            alignment=TA_CENTER
        )
        elements.append(Paragraph("‚îÄ" * 80, footer_style))
        elements.append(Paragraph("Generated by <b>YOLO Object Detection Pro</b>", footer_style))
        elements.append(Paragraph("Built by <b>Samith Shivakumar</b> | Powered by YOLOv11", footer_style))
        elements.append(Paragraph("GitHub: <link href='https://github.com/Sam29W'>Sam29W</link>", footer_style))

        doc.build(elements)

        os.unlink(img_temp.name)

        print(f"‚úÖ PDF report generated: {pdf_path}")
        return pdf_path

    except Exception as e:
        print(f"‚ùå PDF generation error: {e}")
        return None


def play_alert_sound():
    if not SOUND_AVAILABLE:
        print("üîá Sound disabled (cloud deployment)")
        return

    try:
        frequency = 1000
        duration = 500

        sample_rate = 22050
        samples = int(sample_rate * duration / 1000)
        wave = np.sin(2 * np.pi * frequency * np.linspace(0, duration / 1000, samples))
        wave = (wave * 32767).astype(np.int16)

        stereo_wave = np.column_stack((wave, wave))

        sound = pygame.sndarray.make_sound(stereo_wave)
        sound.play()
        time.sleep(duration / 1000)
    except Exception as e:
        print(f"üîá Sound error (expected on cloud): {e}")


def log_alert(alert_message, object_counts, image=None):
    global alert_log

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    alert_entry = {
        'timestamp': timestamp,
        'message': alert_message,
        'object_counts': object_counts
    }

    alert_log.insert(0, alert_entry)

    if len(alert_log) > 50:
        alert_log = alert_log[:50]

    log_file = ALERTS_DIR / f"alert_{timestamp.replace(':', '-').replace(' ', '_')}.json"
    with open(log_file, 'w') as f:
        json.dump(alert_entry, f, indent=2)

    if image is not None:
        img_file = ALERTS_DIR / f"alert_img_{timestamp.replace(':', '-').replace(' ', '_')}.jpg"
        cv2.imwrite(str(img_file), image)


def check_alerts(object_counts, alert_rules):
    triggered_alerts = []

    if alert_rules['person_count_enabled'] and 'person' in object_counts:
        if object_counts['person'] >= alert_rules['person_threshold']:
            triggered_alerts.append(
                f"‚ö†Ô∏è {object_counts['person']} Persons detected! (Threshold: {alert_rules['person_threshold']})")

    if alert_rules['car_alert_enabled'] and 'car' in object_counts:
        triggered_alerts.append(f"üöó Car detected! (Count: {object_counts['car']})")

    if alert_rules['truck_alert_enabled'] and 'truck' in object_counts:
        triggered_alerts.append(f"üöö Truck detected! (Count: {object_counts['truck']})")

    if alert_rules['dog_alert_enabled'] and 'dog' in object_counts:
        triggered_alerts.append(f"üêï Dog detected! (Count: {object_counts['dog']})")

    if alert_rules['cat_alert_enabled'] and 'cat' in object_counts:
        triggered_alerts.append(f"üêà Cat detected! (Count: {object_counts['cat']})")

    if alert_rules['bicycle_alert_enabled'] and 'bicycle' in object_counts:
        triggered_alerts.append(f"üö≤ Bicycle detected! (Count: {object_counts['bicycle']})")

    total_objects = sum(object_counts.values())
    if alert_rules['total_objects_enabled'] and total_objects >= alert_rules['total_objects_threshold']:
        triggered_alerts.append(
            f"üìä {total_objects} Total objects detected! (Threshold: {alert_rules['total_objects_threshold']})")

    return triggered_alerts


def get_alert_log_display():
    if not alert_log:
        return "### üìÇ No Alerts Yet\n\nAlerts will appear here when triggered!"

    display = "# üö® Alert Log (Last 20)\n\n"

    for idx, alert in enumerate(alert_log[:20]):
        display += f"## {idx + 1}. {alert['timestamp']}\n"
        display += f"**{alert['message']}**\n\n"

        if alert['object_counts']:
            display += "**Objects detected:**\n"
            for obj, count in alert['object_counts'].items():
                display += f"- {obj.capitalize()}: {count}\n"

        display += "\n---\n\n"

    return display


def clear_alert_log():
    global alert_log
    alert_log = []

    for file in ALERTS_DIR.glob("*"):
        file.unlink()

    return "### ‚úÖ Alert Log Cleared!\n\nAll alerts have been deleted."


def export_alert_log():
    if not alert_log:
        return None

    export_data = {
        'export_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'total_alerts': len(alert_log),
        'alerts': alert_log
    }

    json_temp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json')
    json.dump(export_data, json_temp, indent=2)
    json_temp.close()

    return json_temp.name


def save_to_history(image, object_counts, timestamp, model_name):
    try:
        img_filename = f"detection_{timestamp.replace(':', '-').replace(' ', '_')}.jpg"
        img_path = HISTORY_DIR / img_filename
        cv2.imwrite(str(img_path), image)

        metadata = {
            'timestamp': timestamp,
            'model': model_name,
            'object_counts': object_counts,
            'total_objects': sum(object_counts.values()),
            'image_path': str(img_path)
        }

        meta_filename = f"metadata_{timestamp.replace(':', '-').replace(' ', '_')}.json"
        meta_path = HISTORY_DIR / meta_filename
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        return True
    except Exception as e:
        print(f"Error saving history: {e}")
        return False


def load_history():
    try:
        history_files = sorted(HISTORY_DIR.glob("metadata_*.json"), reverse=True)

        if not history_files:
            return "### üìÇ No History Yet\n\nStart detecting objects to build your history!"

        history_text = "# üìÇ Detection History\n\n"

        for idx, meta_file in enumerate(history_files[:10]):
            with open(meta_file, 'r') as f:
                data = json.load(f)

            history_text += f"## üïí {data['timestamp']}\n"
            history_text += f"**Model:** {data.get('model', 'yolo11n.pt')}\n"
            history_text += f"**Total Objects:** {data['total_objects']}\n\n"

            for obj, count in sorted(data['object_counts'].items(), key=lambda x: x[1], reverse=True):
                history_text += f"- {obj.capitalize()}: {count}\n"

            history_text += "\n---\n\n"

        return history_text
    except Exception as e:
        return f"### ‚ö†Ô∏è Error Loading History\n\n{str(e)}"


def clear_history():
    try:
        for file in HISTORY_DIR.glob("*"):
            file.unlink()
        return "### ‚úÖ History Cleared!\n\nAll detection history has been deleted."
    except Exception as e:
        return f"### ‚ö†Ô∏è Error: {str(e)}"


def detect_objects_with_alerts(image, confidence, use_custom_colors,
                               person_alert_enabled, person_threshold,
                               car_alert, truck_alert, dog_alert, cat_alert, bicycle_alert,
                               total_objects_enabled, total_threshold,
                               sound_enabled, log_enabled,
                               model_name,
                               *filters):
    if image is None:
        return None, "‚ö†Ô∏è Please upload an image first!", None, None, None, None, None, None

    load_model(model_name)

    filter_classes = [
        'person', 'car', 'truck', 'bus', 'bicycle', 'motorcycle',
        'dog', 'cat', 'bird', 'bottle', 'cup', 'laptop', 'phone', 'chair'
    ]

    selected_filters = [filter_classes[i] for i, f in enumerate(filters) if f]

    start_time = time.time()
    results = current_model.predict(source=image, conf=confidence, save=False)
    process_time = time.time() - start_time

    if use_custom_colors:
        annotated_image = draw_custom_boxes(image, results, confidence)
    else:
        annotated_image = results[0].plot()

    heatmap_pure, heatmap_overlay = generate_heatmap(image, results, confidence)

    detections = []
    object_counts = {}
    export_data = []

    for box in results[0].boxes:
        class_id = int(box.cls[0])
        class_name = results[0].names[class_id]
        conf = float(box.conf[0])
        bbox = box.xyxy[0].tolist()

        if selected_filters and class_name not in selected_filters:
            continue

        detections.append(f"{class_name}: {conf:.2%}")

        if class_name in object_counts:
            object_counts[class_name] += 1
        else:
            object_counts[class_name] = 1

        export_data.append({
            'object': class_name,
            'confidence': f"{conf:.4f}",
            'bbox_x1': round(bbox[0], 2),
            'bbox_y1': round(bbox[1], 2),
            'bbox_x2': round(bbox[2], 2),
            'bbox_y2': round(bbox[3], 2)
        })

    alert_rules = {
        'person_count_enabled': person_alert_enabled,
        'person_threshold': person_threshold,
        'car_alert_enabled': car_alert,
        'truck_alert_enabled': truck_alert,
        'dog_alert_enabled': dog_alert,
        'cat_alert_enabled': cat_alert,
        'bicycle_alert_enabled': bicycle_alert,
        'total_objects_enabled': total_objects_enabled,
        'total_objects_threshold': total_threshold
    }

    triggered_alerts = check_alerts(object_counts, alert_rules)

    alert_summary = ""
    if triggered_alerts:
        alert_summary = "\n\n## üö® ALERTS TRIGGERED!\n\n"

        for alert_msg in triggered_alerts:
            alert_summary += f"### {alert_msg}\n\n"

            if sound_enabled:
                threading.Thread(target=play_alert_sound).start()

            if log_enabled:
                log_alert(alert_msg, object_counts, annotated_image)

        alert_summary += "---\n\n"

    total = len(detections)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    if object_counts:
        save_to_history(annotated_image, object_counts, timestamp, model_name)

    summary = f"# üìä Detection Results\n\n"
    summary += f"**ü§ñ Model:** {model_name}\n"

    if use_custom_colors:
        summary += "**üé® Custom Colors:** ON\n"

    if selected_filters:
        summary += f"**üéØ Filtered to:** {', '.join(selected_filters)}\n"
    else:
        summary += f"**üéØ Filter:** All objects\n"

    summary += f"**Timestamp:** {timestamp}\n"
    summary += f"**‚è±Ô∏è Processing Time:** {process_time:.2f} seconds\n"
    summary += f"**üöÄ Speed:** {1 / process_time:.1f} FPS\n"

    summary += alert_summary

    summary += f"## üéØ Total Objects Found: **{total}**\n\n"

    if total > 0:
        summary += "### üìà Live Object Counter:\n\n"

        emoji_map = {
            'person': 'üë§', 'car': 'üöó', 'truck': 'üöö', 'bus': 'üöå',
            'bicycle': 'üö≤', 'motorcycle': 'üèçÔ∏è', 'dog': 'üêï', 'cat': 'üêà',
            'bird': 'üê¶', 'horse': 'üê¥', 'bottle': 'üçæ', 'cup': '‚òï',
            'laptop': 'üíª', 'phone': 'üì±', 'book': 'üìö', 'chair': 'ü™ë',
            'traffic light': 'üö¶', 'stop sign': 'üõë',
        }

        sorted_objects = sorted(object_counts.items(), key=lambda x: x[1], reverse=True)

        for obj, count in sorted_objects:
            emoji = emoji_map.get(obj, 'üì¶')
            bar = 'üü¶' * count
            summary += f"**{emoji} {obj.capitalize()}:** {count} {bar}\n\n"

        summary += "\n---\n\n### üìã Detailed Detections:\n\n"
        for i, det in enumerate(detections, 1):
            summary += f"{i}. {det}\n"
    else:
        summary = f"# ‚ö†Ô∏è No objects detected!\n\n**Model:** {model_name}\n\n"
        if selected_filters:
            summary += f"**Filter active:** Looking for {', '.join(selected_filters)}\n\n"
        summary += "**Try:** Lowering confidence, changing filter, or using a different image"

    json_file = None
    csv_file = None
    pdf_file = None

    if export_data:
        json_output = {
            'timestamp': timestamp,
            'model': model_name,
            'custom_colors': use_custom_colors,
            'applied_filters': selected_filters if selected_filters else 'all',
            'processing_time_seconds': round(process_time, 2),
            'fps': round(1 / process_time, 2),
            'total_objects': total,
            'object_summary': object_counts,
            'alerts_triggered': triggered_alerts,
            'detections': export_data
        }

        json_temp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json')
        json.dump(json_output, json_temp, indent=2)
        json_temp.close()
        json_file = json_temp.name

        csv_temp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv')
        csv_temp.write("Object,Confidence,BBox_X1,BBox_Y1,BBox_X2,BBox_Y2\n")
        for item in export_data:
            csv_temp.write(
                f"{item['object']},{item['confidence']},{item['bbox_x1']},{item['bbox_y1']},{item['bbox_x2']},{item['bbox_y2']}\n")
        csv_temp.close()
        csv_file = csv_temp.name

        pdf_metadata = {
            'timestamp': timestamp,
            'model': model_name,
            'confidence': confidence,
            'total_objects': total,
            'processing_time': process_time,
            'fps': 1 / process_time
        }

        pdf_file = generate_pdf_report(annotated_image, export_data, object_counts, pdf_metadata)

    alert_log_display = get_alert_log_display()

    return annotated_image, summary, alert_log_display, json_file, csv_file, pdf_file, heatmap_pure, heatmap_overlay


def detect_batch_images(images, confidence, use_custom_colors, model_name, progress=gr.Progress()):
    if not images or len(images) == 0:
        return None, "‚ö†Ô∏è Please upload at least one image!", None

    load_model(model_name)

    progress(0, desc="Starting batch processing...")

    all_results = []
    all_images = []
    total_objects = 0
    combined_counts = {}

    for idx, img_file in enumerate(images):
        progress((idx + 1) / len(images), desc=f"Processing image {idx + 1}/{len(images)}")

        image = Image.open(img_file.name)
        results = current_model.predict(source=image, conf=confidence, save=False)

        if use_custom_colors:
            annotated = draw_custom_boxes(image, results, confidence)
        else:
            annotated = results[0].plot()

        all_images.append(annotated)

        image_counts = {}
        for box in results[0].boxes:
            class_id = int(box.cls[0])
            class_name = results[0].names[class_id]

            if class_name in image_counts:
                image_counts[class_name] += 1
            else:
                image_counts[class_name] = 1

            if class_name in combined_counts:
                combined_counts[class_name] += 1
            else:
                combined_counts[class_name] = 1

            total_objects += 1

        all_results.append({
            'image_number': idx + 1,
            'objects_found': len(results[0].boxes),
            'object_breakdown': image_counts
        })

    summary = f"# üì¶ Batch Processing Complete!\n\n"
    summary += f"**ü§ñ Model:** {model_name}\n"
    summary += f"**Total Images Processed:** {len(images)}\n"
    summary += f"**Total Objects Found:** {total_objects}\n\n"
    summary += f"### üìä Combined Object Counts:\n\n"

    for obj, count in sorted(combined_counts.items(), key=lambda x: x[1], reverse=True):
        summary += f"- **{obj.capitalize()}:** {count}\n"

    summary += f"\n### üìã Per-Image Results:\n\n"
    for result in all_results:
        summary += f"**Image {result['image_number']}:** {result['objects_found']} objects\n"
        for obj, count in result['object_breakdown'].items():
            summary += f"  - {obj}: {count}\n"
        summary += "\n"

    zip_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
    with zipfile.ZipFile(zip_temp.name, 'w') as zipf:
        for idx, img in enumerate(all_images):
            img_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
            cv2.imwrite(img_temp.name, img)
            zipf.write(img_temp.name, f'detected_image_{idx + 1}.jpg')
            os.unlink(img_temp.name)

    progress(1.0, desc="Done!")

    return all_images[0] if all_images else None, summary, zip_temp.name


def detect_objects_video_with_frames(video, confidence, model_name, progress=gr.Progress()):
    if video is None:
        return None, "‚ö†Ô∏è Please upload a video first!", None, None, None

    load_model(model_name)

    start_time = time.time()
    progress(0, desc="Starting video processing...")

    cap = cv2.VideoCapture(video)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    temp_output = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    output_path = temp_output.name

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    all_detections = {}
    video_export_data = []
    frame_detection_counts = []
    key_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        progress(frame_count / total_frames, desc=f"Processing frame {frame_count}/{total_frames}")

        results = current_model.predict(source=frame, conf=confidence, save=False, verbose=False)
        annotated_frame = results[0].plot()

        num_detections = len(results[0].boxes)
        frame_detection_counts.append((frame_count, num_detections))

        if num_detections > 0 and len(key_frames) < 5:
            key_frame_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
            cv2.imwrite(key_frame_temp.name, annotated_frame)
            key_frames.append((frame_count, num_detections, key_frame_temp.name))

        for box in results[0].boxes:
            class_id = int(box.cls[0])
            class_name = results[0].names[class_id]
            conf = float(box.conf[0])

            if class_name in all_detections:
                all_detections[class_name] += 1
            else:
                all_detections[class_name] = 1

            video_export_data.append({
                'frame': frame_count,
                'object': class_name,
                'confidence': f"{conf:.4f}"
            })

        out.write(annotated_frame)

    cap.release()
    out.release()

    total_time = time.time() - start_time
    avg_fps = frame_count / total_time if total_time > 0 else 0

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    total_detections = sum(all_detections.values())

    max_detections_frame = max(frame_detection_counts, key=lambda x: x[1]) if frame_detection_counts else (0, 0)

    summary = f"# üé• Video Processing Complete!\n\n"
    summary += f"**ü§ñ Model:** {model_name}\n"
    summary += f"**Timestamp:** {timestamp}\n"
    summary += f"**‚è±Ô∏è Total Processing Time:** {total_time:.2f} seconds\n"
    summary += f"**üöÄ Average Speed:** {avg_fps:.1f} FPS\n"
    summary += f"**üìπ Total Frames:** {frame_count}\n"
    summary += f"**üéØ Total Detections:** {total_detections}\n"
    summary += f"**üìä Peak Frame:** #{max_detections_frame[0]} ({max_detections_frame[1]} objects)\n\n"

    if all_detections:
        summary += "### üìä Objects Detected:\n\n"
        emoji_map = {
            'person': 'üë§', 'car': 'üöó', 'truck': 'üöö', 'bus': 'üöå',
            'dog': 'üêï', 'cat': 'üêà', 'bicycle': 'üö≤', 'motorcycle': 'üèçÔ∏è'
        }

        for obj, count in sorted(all_detections.items(), key=lambda x: x[1], reverse=True):
            emoji = emoji_map.get(obj, 'üì¶')
            bar = 'üü©' * min(count // 10, 20)
            summary += f"**{emoji} {obj.capitalize()}:** {count} times {bar}\n\n"

        summary += f"\n### üé¨ Key Frames Extracted: {len(key_frames)}\n"
        for frame_num, num_det, _ in key_frames:
            summary += f"- Frame #{frame_num}: {num_det} objects\n"
    else:
        summary = f"### ‚ö†Ô∏è No objects detected!\n**Model:** {model_name}"

    progress(1.0, desc="Done!")

    json_file = None
    csv_file = None

    if video_export_data:
        json_output = {
            'timestamp': timestamp,
            'model': model_name,
            'processing_time_seconds': round(total_time, 2),
            'average_fps': round(avg_fps, 2),
            'total_frames': frame_count,
            'total_detections': total_detections,
            'peak_frame': max_detections_frame[0],
            'peak_detections': max_detections_frame[1],
            'object_summary': all_detections,
            'frame_detections': video_export_data
        }

        json_temp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json')
        json.dump(json_output, json_temp, indent=2)
        json_temp.close()
        json_file = json_temp.name

        csv_temp = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv')
        csv_temp.write("Frame,Object,Confidence\n")
        for item in video_export_data:
            csv_temp.write(f"{item['frame']},{item['object']},{item['confidence']}\n")
        csv_temp.close()
        csv_file = csv_temp.name

    preview_frame = key_frames[0][2] if key_frames else None

    return output_path, summary, preview_frame, json_file, csv_file


frame_times = []


def detect_webcam(image, confidence, model_name):
    global frame_times

    if image is None:
        return None, "### üéØ Waiting for webcam..."

    load_model(model_name)

    current_time = time.time()
    frame_times.append(current_time)

    frame_times = [t for t in frame_times if current_time - t < 1.0]
    fps = len(frame_times)

    results = current_model.predict(source=image, conf=confidence, save=False, verbose=False)
    annotated_image = results[0].plot()

    object_counts = {}
    for box in results[0].boxes:
        class_id = int(box.cls[0])
        class_name = results[0].names[class_id]

        if class_name in object_counts:
            object_counts[class_name] += 1
        else:
            object_counts[class_name] = 1

    counter_text = "### üî¥ LIVE Detection\n\n"
    counter_text += f"**ü§ñ Model:** {model_name}\n"
    counter_text += f"**üöÄ FPS: {fps}** | **‚è±Ô∏è Latency: {1000 / fps if fps > 0 else 0:.0f}ms**\n\n"

    if object_counts:
        emoji_map = {
            'person': 'üë§', 'car': 'üöó', 'truck': 'üöö', 'bus': 'üöå',
            'dog': 'üêï', 'cat': 'üêà', 'bicycle': 'üö≤', 'motorcycle': 'üèçÔ∏è',
            'bottle': 'üçæ', 'cup': '‚òï', 'laptop': 'üíª', 'phone': 'üì±'
        }

        total = sum(object_counts.values())
        counter_text += f"**üéØ Total: {total} objects**\n\n"

        for obj, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
            emoji = emoji_map.get(obj, 'üì¶')
            counter_text += f"{emoji} **{obj.capitalize()}: {count}** | "

        counter_text = counter_text.rstrip(" | ")
    else:
        counter_text += "*No objects detected!*"

    return annotated_image, counter_text


# Create Gradio interface
with gr.Blocks(title="YOLO Object Detection Pro - Multi-Model + Mobile + Tracking") as demo:
    gr.Markdown("# üöÄ YOLO Object Detection Pro - **WITH TRACKING!**")
    gr.Markdown(
        "### AI-powered detection with **Multi-Model**, **Mobile Camera**, **QR Sharing**, **OBJECT TRACKING** üéØ, custom colors, batch processing, video analysis, history, **SMART ALERTS**, **PDF Reports** & **üó∫Ô∏è HEATMAPS**!")

    # GLOBAL MODEL SELECTOR
    with gr.Row():
        with gr.Column(scale=3):
            global_model_selector = gr.Dropdown(
                choices=AVAILABLE_MODELS,
                value="yolo11n.pt",
                label="ü§ñ Select YOLO Model (Global)",
                info="n=fastest ‚ö° | s=fast üèÉ | m=balanced ‚öñÔ∏è | l=accurate üéØ | x=most accurate üî•"
            )
        with gr.Column(scale=2):
            gr.Markdown("""
            ### Model Guide:
            - **n**: 2.6M params, fastest
            - **s**: 9.4M params, very fast
            - **m**: 20.1M params, balanced
            - **l**: 25.3M params, accurate
            - **x**: 56.9M params, best accuracy
            """)

    with gr.Tabs():
        # Image Detection Tab WITH ALERTS
        with gr.Tab("üì∑ Image Detection + Alerts"):
            with gr.Row():
                with gr.Column():
                    image_input = gr.Image(type="pil", label="üì§ Upload Image")
                    image_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence Threshold")

                    custom_colors_toggle = gr.Checkbox(label="üé® Use Custom Colors", value=True,
                                                       info="Different color per object type")

                    # ALERT SETTINGS
                    with gr.Accordion("üîî Smart Alert Settings", open=True):
                        gr.Markdown("**Configure detection alerts:**")

                        with gr.Row():
                            person_alert_enabled = gr.Checkbox(label="Alert on Person Count", value=False)
                            person_threshold = gr.Slider(1, 20, 3, step=1, label="Person Threshold")

                        with gr.Row():
                            total_objects_enabled = gr.Checkbox(label="Alert on Total Objects", value=False)
                            total_threshold = gr.Slider(1, 50, 10, step=1, label="Total Objects Threshold")

                        gr.Markdown("**Specific Object Alerts:**")
                        with gr.Row():
                            car_alert = gr.Checkbox(label="üöó Alert on Car", value=False)
                            truck_alert = gr.Checkbox(label="üöö Alert on Truck", value=False)
                            bicycle_alert = gr.Checkbox(label="üö≤ Alert on Bicycle", value=False)

                        with gr.Row():
                            dog_alert = gr.Checkbox(label="üêï Alert on Dog", value=False)
                            cat_alert = gr.Checkbox(label="üêà Alert on Cat", value=False)

                        gr.Markdown("**Notification Methods:**")
                        with gr.Row():
                            sound_enabled = gr.Checkbox(label="üîä Sound Alert", value=True,
                                                        info="Auto-disabled on cloud")
                            log_enabled = gr.Checkbox(label="üìù Log Alert", value=True)

                    # Object filter checkboxes
                    with gr.Accordion("üéØ Filter Objects (Optional)", open=False):
                        gr.Markdown("**Select specific objects:**")

                        filter_person = gr.Checkbox(label="üë§ Person", value=False)
                        filter_car = gr.Checkbox(label="üöó Car", value=False)
                        filter_truck = gr.Checkbox(label="üöö Truck", value=False)
                        filter_bus = gr.Checkbox(label="üöå Bus", value=False)
                        filter_bicycle = gr.Checkbox(label="üö≤ Bicycle", value=False)
                        filter_motorcycle = gr.Checkbox(label="üèçÔ∏è Motorcycle", value=False)
                        filter_dog = gr.Checkbox(label="üêï Dog", value=False)
                        filter_cat = gr.Checkbox(label="üêà Cat", value=False)
                        filter_bird = gr.Checkbox(label="üê¶ Bird", value=False)
                        filter_bottle = gr.Checkbox(label="üçæ Bottle", value=False)
                        filter_cup = gr.Checkbox(label="‚òï Cup", value=False)
                        filter_laptop = gr.Checkbox(label="üíª Laptop", value=False)
                        filter_phone = gr.Checkbox(label="üì± Phone", value=False)
                        filter_chair = gr.Checkbox(label="ü™ë Chair", value=False)

                    image_btn = gr.Button("üîç Detect with Alerts", variant="primary", size="lg")

                    gr.Examples(
                        examples=[
                            ["https://ultralytics.com/images/bus.jpg"],
                            ["https://ultralytics.com/images/zidane.jpg"],
                        ],
                        inputs=image_input,
                        label="üì∏ Examples"
                    )

                with gr.Column():
                    image_output = gr.Image(type="numpy", label="‚ú® Detection Results")
                    image_text = gr.Markdown()

                    # Heatmap visualizations
                    with gr.Accordion("üó∫Ô∏è Heatmap Visualization", open=False):
                        with gr.Row():
                            heatmap_pure = gr.Image(type="numpy", label="üî• Pure Heatmap")
                            heatmap_overlay = gr.Image(type="numpy", label="üé® Heatmap Overlay")
                        gr.Markdown("""
                        **Heatmap Guide:**
                        - üîµ Blue = Low detection density
                        - üü¢ Green = Medium density
                        - üü° Yellow = High density
                        - üî¥ Red = Highest density (hotspots)
                        """)

                    alert_log_display = gr.Markdown("### üìÇ Alert Log\n\nAlerts will appear here!")

                    with gr.Row():
                        json_download = gr.File(label="üìÑ JSON")
                        csv_download = gr.File(label="üìä CSV")
                        pdf_download = gr.File(label="üìã PDF Report")

            image_btn.click(
                fn=detect_objects_with_alerts,
                inputs=[
                    image_input, image_confidence, custom_colors_toggle,
                    person_alert_enabled, person_threshold,
                    car_alert, truck_alert, dog_alert, cat_alert, bicycle_alert,
                    total_objects_enabled, total_threshold,
                    sound_enabled, log_enabled,
                    global_model_selector,
                    filter_person, filter_car, filter_truck, filter_bus,
                    filter_bicycle, filter_motorcycle, filter_dog, filter_cat,
                    filter_bird, filter_bottle, filter_cup, filter_laptop,
                    filter_phone, filter_chair
                ],
                outputs=[image_output, image_text, alert_log_display, json_download, csv_download, pdf_download,
                         heatmap_pure, heatmap_overlay]
            )

        # üéØ OBJECT TRACKING TAB - THE NEW FEATURE!
        with gr.Tab("üéØ Object Tracking"):
            gr.Markdown("# üéØ Real-Time Object Tracking with Trajectories")
            gr.Markdown("### Track objects across video frames with unique IDs and movement trails!")

            with gr.Row():
                with gr.Column():
                    tracking_video_input = gr.Video(label="üì§ Upload Video for Tracking")
                    tracking_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence Threshold")
                    tracking_trails = gr.Checkbox(label="üé® Draw Trajectory Trails", value=True,
                                                  info="Show colorful movement paths")
                    tracking_btn = gr.Button("üéØ Track Objects", variant="primary", size="lg")

                    gr.Markdown("""
                    ### ‚ú® Tracking Features:
                    - üÜî **Unique ID Assignment** - Each object gets tracked ID
                    - üé® **Trajectory Visualization** - See movement paths
                    - üìä **Track Statistics** - Duration, frames tracked
                    - üéØ **Persistent Tracking** - Follows objects across frames
                    - üåà **Color-Coded IDs** - Easy visual identification

                    ### üíº Use Cases:
                    - üöó **Traffic Analysis**: Track vehicle routes
                    - üè™ **Retail**: Customer movement patterns
                    - üèÉ **Sports**: Player positioning & speed
                    - üè¢ **Security**: Monitor suspicious behavior
                    - üêæ **Wildlife**: Animal migration tracking
                    """)

                with gr.Column():
                    tracking_output = gr.Video(label="üéØ Tracked Video with IDs & Trails")
                    tracking_summary = gr.Markdown("### üìä Tracking results will appear here!")

                    gr.Markdown("""
                    **What You Get:**
                    - Each object has unique ID number
                    - Colorful trails show movement history
                    - Statistics on tracking duration
                    - Total unique objects counted

                    **üí° Pro Tip:** Lower confidence = more detections but may lose tracks. Higher confidence = stable tracking!
                    """)

            tracking_btn.click(
                fn=track_objects_in_video,
                inputs=[tracking_video_input, tracking_confidence, global_model_selector, tracking_trails],
                outputs=[tracking_output, tracking_summary]
            )

        # Mobile Camera Tab üì±
        with gr.Tab("üì± Mobile Camera"):
            gr.Markdown("# üì± Mobile-Optimized Detection")
            gr.Markdown("### üéØ Perfect for smartphones! Larger buttons, touch-friendly interface")

            with gr.Row():
                with gr.Column(scale=1):
                    mobile_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="üìä Confidence")

                    gr.Markdown("""
                    ### üì∏ How to Use:
                    1. üì∑ **Tap camera button** below
                    2. üéØ **Take photo** or choose from gallery
                    3. ‚ö° **Instant detection** results!
                    4. üíæ **Download** annotated image
                    """)

                    gr.Markdown("""
                    ### ‚ú® Mobile Features:
                    - üì± **Native camera access**
                    - üëÜ **Touch-optimized UI**
                    - üîç **Larger detection boxes**
                    - üì• **Easy sharing**
                    - ‚ö° **Fast processing**
                    """)

            with gr.Row():
                mobile_camera_input = gr.Image(
                    sources=["upload", "webcam"],
                    type="numpy",
                    label="üì∑ Capture or Upload Photo",
                    height=400
                )

            mobile_detect_btn = gr.Button(
                "üîç DETECT OBJECTS",
                variant="primary",
                size="lg",
                scale=2
            )

            mobile_output = gr.Image(type="numpy", label="‚ú® Results", height=400)
            mobile_summary = gr.Markdown("### üì± Results will appear here!")
            mobile_download = gr.File(label="üíæ Download Result")

            mobile_detect_btn.click(
                fn=detect_mobile_camera,
                inputs=[mobile_camera_input, mobile_confidence, global_model_selector],
                outputs=[mobile_output, mobile_summary, mobile_download]
            )

            gr.Markdown("""
            ---
            ### üí° Pro Tips:
            - üì∑ Use **good lighting** for best results
            - üéØ **Center objects** in frame
            - üìè Keep **optimal distance** (not too close/far)
            - üîÑ Try different **angles** if needed
            """)

        # QR Code Sharing Tab
        with gr.Tab("üì≤ Share via QR"):
            gr.Markdown("# üì≤ Share This App!")
            gr.Markdown("### Scan QR code to open on any device")

            with gr.Row():
                with gr.Column():
                    share_url_input = gr.Textbox(
                        label="üîó App URL",
                        placeholder="Enter your HuggingFace Space URL or localhost URL",
                        value="https://huggingface.co/spaces/Samith29/yolo-object-detection",
                        scale=3
                    )
                    qr_generate_btn = gr.Button("üé® Generate QR Code", variant="primary", size="lg")

                    gr.Markdown("""
                    ### üì± Use Cases:
                    - Share with team members
                    - Easy mobile access
                    - Demo presentations
                    - Quick testing on multiple devices
                    """)

                with gr.Column():
                    qr_output = gr.Image(type="numpy", label="üì≤ Scan Me!")
                    gr.Markdown("""
                    **How to use:**
                    1. Open camera app on phone
                    2. Point at QR code
                    3. Tap notification to open
                    4. Start detecting! üöÄ
                    """)

            qr_generate_btn.click(
                fn=generate_qr_code,
                inputs=[share_url_input],
                outputs=[qr_output]
            )

        # Batch Processing Tab
        with gr.Tab("üì¶ Batch Processing"):
            with gr.Row():
                with gr.Column():
                    batch_images = gr.File(file_count="multiple", label="üì§ Upload Multiple Images",
                                           file_types=["image"])
                    batch_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence")
                    batch_colors = gr.Checkbox(label="üé® Use Custom Colors", value=True)
                    batch_btn = gr.Button("üöÄ Process Batch", variant="primary", size="lg")

                    gr.Markdown("""
                    **Batch Processing:**
                    - Upload multiple images at once
                    - Compare models on same images
                    - Download zip with all results
                    """)

                with gr.Column():
                    batch_preview = gr.Image(type="numpy", label="Preview (First Image)")
                    batch_text = gr.Markdown()
                    batch_zip = gr.File(label="üì¶ Download All Results (ZIP)")

            batch_btn.click(
                fn=detect_batch_images,
                inputs=[batch_images, batch_confidence, batch_colors, global_model_selector],
                outputs=[batch_preview, batch_text, batch_zip]
            )

        # Video Detection Tab
        with gr.Tab("üé• Video Detection"):
            with gr.Row():
                with gr.Column():
                    video_input = gr.Video(label="üì§ Upload Video")
                    video_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence")
                    video_btn = gr.Button("üé¨ Process Video", variant="primary", size="lg")
                    gr.Markdown("**Note:** Extracts key frames with most detections!")

                with gr.Column():
                    video_output = gr.Video(label="‚ú® Detected Video")
                    video_text = gr.Markdown()

                    key_frame_preview = gr.Image(type="filepath", label="üé¨ Key Frame Preview")

                    with gr.Row():
                        video_json = gr.File(label="üìÑ JSON")
                        video_csv = gr.File(label="üìä CSV")

            video_btn.click(
                fn=detect_objects_video_with_frames,
                inputs=[video_input, video_confidence, global_model_selector],
                outputs=[video_output, video_text, key_frame_preview, video_json, video_csv]
            )

        # Video Heatmap Tab
        with gr.Tab("üó∫Ô∏è Video Heatmap"):
            gr.Markdown("### üî• Generate Detection Heatmap from Video")
            gr.Markdown("**Shows hotspots where objects are detected most frequently across all frames!**")

            with gr.Row():
                with gr.Column():
                    heatmap_video_input = gr.Video(label="üì§ Upload Video")
                    heatmap_video_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence Threshold")
                    heatmap_video_btn = gr.Button("üó∫Ô∏è Generate Heatmap", variant="primary", size="lg")

                    gr.Markdown("""
                    **Use Cases:**
                    - üè¢ Security: Find high-traffic areas
                    - üöó Traffic: Identify busy lanes
                    - üõí Retail: Customer movement patterns
                    - ‚öΩ Sports: Player positioning analysis
                    """)

                with gr.Column():
                    with gr.Row():
                        heatmap_video_pure = gr.Image(type="numpy", label="üî• Pure Heatmap")
                        heatmap_video_overlay = gr.Image(type="numpy", label="üé® Overlay")

                    gr.Markdown("""
                    **Legend:**
                    - üîµ **Blue**: Low activity
                    - üü¢ **Green**: Moderate activity
                    - üü° **Yellow**: High activity
                    - üî¥ **Red**: Hotspot (highest activity)
                    """)

            heatmap_video_btn.click(
                fn=generate_video_heatmap,
                inputs=[heatmap_video_input, heatmap_video_confidence, global_model_selector],
                outputs=[heatmap_video_pure, heatmap_video_overlay]
            )

        # Webcam Tab
        with gr.Tab("üì∏ Live Webcam"):
            gr.Markdown("### üî¥ Real-Time Detection")
            gr.Markdown("**Click 'Start Webcam' and allow camera access!**")

            with gr.Row():
                with gr.Column():
                    webcam_confidence = gr.Slider(0.1, 0.95, 0.5, step=0.05, label="Confidence")
                    webcam_counter = gr.Markdown("### üéØ Waiting...")

                    gr.Markdown("**Tips:** Good lighting, center objects, adjust confidence")

                with gr.Column():
                    webcam_output = gr.Image(
                        sources=["webcam"],
                        streaming=True,
                        type="numpy",
                        label="üé• Live Feed"
                    )

            webcam_output.stream(
                fn=detect_webcam,
                inputs=[webcam_output, webcam_confidence, global_model_selector],
                outputs=[webcam_output, webcam_counter],
                time_limit=60,
                stream_every=0.1
            )

        # Alert Log Tab
        with gr.Tab("üö® Alert Log"):
            gr.Markdown("### üö® Alert History & Management")
            gr.Markdown("View and manage all triggered alerts!")

            with gr.Row():
                alert_refresh_btn = gr.Button("üîÑ Refresh Log", variant="primary")
                alert_clear_btn = gr.Button("üóëÔ∏è Clear All Alerts", variant="stop")
                alert_export_btn = gr.Button("üì• Export Log", variant="secondary")

            alert_history_display = gr.Markdown("Click 'Refresh Log' to view alerts!")
            alert_export_file = gr.File(label="üìÑ Download Alert Log")

            alert_refresh_btn.click(fn=get_alert_log_display, outputs=alert_history_display)
            alert_clear_btn.click(fn=clear_alert_log, outputs=alert_history_display)
            alert_export_btn.click(fn=export_alert_log, outputs=alert_export_file)

        # History Tab
        with gr.Tab("üìÇ Detection History"):
            gr.Markdown("### üìÇ Detection History")
            gr.Markdown("View all your past detections with timestamps, models, and object counts!")

            with gr.Row():
                history_load_btn = gr.Button("üîÑ Load History", variant="primary")
                history_clear_btn = gr.Button("üóëÔ∏è Clear History", variant="stop")

            history_display = gr.Markdown("Click 'Load History' to view your past detections!")

            history_load_btn.click(fn=load_history, outputs=history_display)
            history_clear_btn.click(fn=clear_history, outputs=history_display)

    gr.Markdown("---")
    gr.Markdown("Built by **Samith Shivakumar** | Powered by YOLOv11 üöÄ")
    gr.Markdown(
        "‚≠ê **Features:** ü§ñ **Multi-Model** | üé® Custom Colors | üì¶ Batch | üé¨ Video Analysis | üì∏ Webcam | üìÇ History | üîî **Smart Alerts** | üìã **PDF Reports** | üó∫Ô∏è **Heatmaps** | üì± **Mobile-Ready** | üì≤ **QR Sharing** | üéØ **OBJECT TRACKING**")

if __name__ == "__main__":
    demo.launch()
